# Milestone 2
Note: All artefacts should have a clearly visible indication of who authored them and who reviewed them.
## Design patterns identification
The team members should produce a report with a collection of design patterns identified
within the code base. The design patterns shall be identified from those discussed in class. The
identification of the design patterns must include:
- An illustrating code snippet
- A Class diagram similar to the patterns presented in class, but representing the particular instance of the pattern identified (including the specific pattern participants).
- The exact location on the codebase (e.g., package, class, method(s))
- A discussion of the rationale for identifying this as a pattern instance.

Each team member is responsible for identifying three different non-trivial design patterns.
Design patterns may overlap from one team member to another as long as they correspond
to different locations in the code. They should also review three other design patterns
(ideally from 3 different colleagues). In general, these reviews should prioritise diversity. 
In other words, rather than having always the same reviewers for the same authors, reviewers 
should rotate for the different deliverables so that all team members get to review some of 
the work of all their colleagues. This principle applies to all project deliverables and 
promotes co-ownership of the project deliverables, which is a crucial element in our scrum-based approach.

## Codebase metrics assessment
The team should produce a report with a metrics-based overview of the codebase. The team
must select metrics sets from those available in the MetricsReloaded plugin for IntelliJ IDEA.
This overview should include:
- A data file with the collected metrics.
- A report explaining the collected metrics.
- Identification of potential trouble spots in the codebase. (Hint: look for the extreme values in the collected metrics - boxplots, or other data visualisation techniques are useful here).
- A short discussion on how these metrics might relate to the identified code smells identified, where applicable.

MetricsReloaded offers many metrics sets. Use one metrics set per team member, chosen from
the following. Choose from: Chidamber and Kemerer, Complexity Metrics, Dependency Metrics, Lines of Code Metrics, MOOD metrics, and Martin Packaging Metrics. As usual, each
team member should review the deliverables of another team member.

## Code smells report
The team members should produce a report containing the identified code smells. This identification must include:
- Illustrating code snippet.
- The exact location on the codebase.
- An explanation of the rationale for identifying this code smell (e.g. where applicable, a reference to the code metrics which helped identify the smell).
- The proposal for a refactoring that would remove the smell.

Each team member is responsible for identifying three different code smells. They should also
review three other code smells (ideally from 3 different colleagues).

## Use case diagrams
The team members should produce use case diagrams for the project, along with 
a summary description of all the actors and 
use cases. For the use cases, please report their name, description and actors
(both primary and secondary). Note that you can and should divide the diagram thematically
into different sub-diagrams so that each use case sub-diagram only shows a subset of related
use cases. This is to avoid building a massive use case diagram that would be too complex to
discuss. This also allows a fair division of responsibilities within the team.

Each team member should be responsible for building at least one of these
diagrams and reviewing a different one.
